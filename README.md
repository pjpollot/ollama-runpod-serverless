# ollama for runpod serverless

A basic template to run a LLM with RunPod serverless.

## Build

```sh
docker build -t <image name> . [--build-arg MODEL_NAME=<model name>]
```